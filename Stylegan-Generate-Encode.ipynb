{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbpresent": {
     "id": "6889ab8d-0a22-4bba-8dc0-d5b03d23c844"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import PIL.Image\n",
    "import PIL.ImageSequence\n",
    "import dnnlib\n",
    "import dnnlib.tflib as tflib\n",
    "from IPython.display import display, clear_output\n",
    "import moviepy\n",
    "import moviepy.editor\n",
    "import math\n",
    "import glob\n",
    "import csv\n",
    "from functools import partial\n",
    "import time\n",
    "import collections\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "import colorsys\n",
    "import requests\n",
    "import re\n",
    "import copy\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbpresent": {
     "id": "d32e7195-2958-409b-b272-7284e737ec20"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gs                            Params    OutputShape         WeightShape     \n",
      "---                           ---       ---                 ---             \n",
      "latents_in                    -         (?, 512)            -               \n",
      "labels_in                     -         (?, 0)              -               \n",
      "lod                           -         ()                  -               \n",
      "dlatent_avg                   -         (512,)              -               \n",
      "G_mapping/latents_in          -         (?, 512)            -               \n",
      "G_mapping/labels_in           -         (?, 0)              -               \n",
      "G_mapping/PixelNorm           -         (?, 512)            -               \n",
      "G_mapping/Dense0              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense1              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense2              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense3              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense4              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense5              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense6              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Dense7              262656    (?, 512)            (512, 512)      \n",
      "G_mapping/Broadcast           -         (?, 16, 512)        -               \n",
      "G_mapping/dlatents_out        -         (?, 16, 512)        -               \n",
      "Truncation                    -         (?, 16, 512)        -               \n",
      "G_synthesis/dlatents_in       -         (?, 16, 512)        -               \n",
      "G_synthesis/4x4/Const         534528    (?, 512, 4, 4)      (512,)          \n",
      "G_synthesis/4x4/Conv          2885632   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
      "G_synthesis/ToRGB_lod7        1539      (?, 3, 4, 4)        (1, 1, 512, 3)  \n",
      "G_synthesis/8x8/Conv0_up      2885632   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "G_synthesis/8x8/Conv1         2885632   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "G_synthesis/ToRGB_lod6        1539      (?, 3, 8, 8)        (1, 1, 512, 3)  \n",
      "G_synthesis/Upscale2D         -         (?, 3, 8, 8)        -               \n",
      "G_synthesis/Grow_lod6         -         (?, 3, 8, 8)        -               \n",
      "G_synthesis/16x16/Conv0_up    2885632   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "G_synthesis/16x16/Conv1       2885632   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "G_synthesis/ToRGB_lod5        1539      (?, 3, 16, 16)      (1, 1, 512, 3)  \n",
      "G_synthesis/Upscale2D_1       -         (?, 3, 16, 16)      -               \n",
      "G_synthesis/Grow_lod5         -         (?, 3, 16, 16)      -               \n",
      "G_synthesis/32x32/Conv0_up    2885632   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "G_synthesis/32x32/Conv1       2885632   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "G_synthesis/ToRGB_lod4        1539      (?, 3, 32, 32)      (1, 1, 512, 3)  \n",
      "G_synthesis/Upscale2D_2       -         (?, 3, 32, 32)      -               \n",
      "G_synthesis/Grow_lod4         -         (?, 3, 32, 32)      -               \n",
      "G_synthesis/64x64/Conv0_up    1442816   (?, 256, 64, 64)    (3, 3, 512, 256)\n",
      "G_synthesis/64x64/Conv1       852992    (?, 256, 64, 64)    (3, 3, 256, 256)\n",
      "G_synthesis/ToRGB_lod3        771       (?, 3, 64, 64)      (1, 1, 256, 3)  \n",
      "G_synthesis/Upscale2D_3       -         (?, 3, 64, 64)      -               \n",
      "G_synthesis/Grow_lod3         -         (?, 3, 64, 64)      -               \n",
      "G_synthesis/128x128/Conv0_up  426496    (?, 128, 128, 128)  (3, 3, 256, 128)\n",
      "G_synthesis/128x128/Conv1     279040    (?, 128, 128, 128)  (3, 3, 128, 128)\n",
      "G_synthesis/ToRGB_lod2        387       (?, 3, 128, 128)    (1, 1, 128, 3)  \n",
      "G_synthesis/Upscale2D_4       -         (?, 3, 128, 128)    -               \n",
      "G_synthesis/Grow_lod2         -         (?, 3, 128, 128)    -               \n",
      "G_synthesis/256x256/Conv0_up  139520    (?, 64, 256, 256)   (3, 3, 128, 64) \n",
      "G_synthesis/256x256/Conv1     102656    (?, 64, 256, 256)   (3, 3, 64, 64)  \n",
      "G_synthesis/ToRGB_lod1        195       (?, 3, 256, 256)    (1, 1, 64, 3)   \n",
      "G_synthesis/Upscale2D_5       -         (?, 3, 256, 256)    -               \n",
      "G_synthesis/Grow_lod1         -         (?, 3, 256, 256)    -               \n",
      "G_synthesis/512x512/Conv0_up  51328     (?, 32, 512, 512)   (3, 3, 64, 32)  \n",
      "G_synthesis/512x512/Conv1     42112     (?, 32, 512, 512)   (3, 3, 32, 32)  \n",
      "G_synthesis/ToRGB_lod0        99        (?, 3, 512, 512)    (1, 1, 32, 3)   \n",
      "G_synthesis/Upscale2D_6       -         (?, 3, 512, 512)    -               \n",
      "G_synthesis/Grow_lod0         -         (?, 3, 512, 512)    -               \n",
      "G_synthesis/images_out        -         (?, 3, 512, 512)    -               \n",
      "G_synthesis/lod               -         ()                  -               \n",
      "G_synthesis/noise0            -         (1, 1, 4, 4)        -               \n",
      "G_synthesis/noise1            -         (1, 1, 4, 4)        -               \n",
      "G_synthesis/noise2            -         (1, 1, 8, 8)        -               \n",
      "G_synthesis/noise3            -         (1, 1, 8, 8)        -               \n",
      "G_synthesis/noise4            -         (1, 1, 16, 16)      -               \n",
      "G_synthesis/noise5            -         (1, 1, 16, 16)      -               \n",
      "G_synthesis/noise6            -         (1, 1, 32, 32)      -               \n",
      "G_synthesis/noise7            -         (1, 1, 32, 32)      -               \n",
      "G_synthesis/noise8            -         (1, 1, 64, 64)      -               \n",
      "G_synthesis/noise9            -         (1, 1, 64, 64)      -               \n",
      "G_synthesis/noise10           -         (1, 1, 128, 128)    -               \n",
      "G_synthesis/noise11           -         (1, 1, 128, 128)    -               \n",
      "G_synthesis/noise12           -         (1, 1, 256, 256)    -               \n",
      "G_synthesis/noise13           -         (1, 1, 256, 256)    -               \n",
      "G_synthesis/noise14           -         (1, 1, 512, 512)    -               \n",
      "G_synthesis/noise15           -         (1, 1, 512, 512)    -               \n",
      "images_out                    -         (?, 3, 512, 512)    -               \n",
      "---                           ---       ---                 ---             \n",
      "Total                         26179768                                      \n",
      "\n",
      "\n",
      "D                    Params    OutputShape         WeightShape     \n",
      "---                  ---       ---                 ---             \n",
      "images_in            -         (?, 3, 512, 512)    -               \n",
      "labels_in            -         (?, 0)              -               \n",
      "lod                  -         ()                  -               \n",
      "FromRGB_lod0         128       (?, 32, 512, 512)   (1, 1, 3, 32)   \n",
      "512x512/Conv0        9248      (?, 32, 512, 512)   (3, 3, 32, 32)  \n",
      "512x512/Conv1_down   18496     (?, 64, 256, 256)   (3, 3, 32, 64)  \n",
      "Downscale2D          -         (?, 3, 256, 256)    -               \n",
      "FromRGB_lod1         256       (?, 64, 256, 256)   (1, 1, 3, 64)   \n",
      "Grow_lod0            -         (?, 64, 256, 256)   -               \n",
      "256x256/Conv0        36928     (?, 64, 256, 256)   (3, 3, 64, 64)  \n",
      "256x256/Conv1_down   73856     (?, 128, 128, 128)  (3, 3, 64, 128) \n",
      "Downscale2D_1        -         (?, 3, 128, 128)    -               \n",
      "FromRGB_lod2         512       (?, 128, 128, 128)  (1, 1, 3, 128)  \n",
      "Grow_lod1            -         (?, 128, 128, 128)  -               \n",
      "128x128/Conv0        147584    (?, 128, 128, 128)  (3, 3, 128, 128)\n",
      "128x128/Conv1_down   295168    (?, 256, 64, 64)    (3, 3, 128, 256)\n",
      "Downscale2D_2        -         (?, 3, 64, 64)      -               \n",
      "FromRGB_lod3         1024      (?, 256, 64, 64)    (1, 1, 3, 256)  \n",
      "Grow_lod2            -         (?, 256, 64, 64)    -               \n",
      "64x64/Conv0          590080    (?, 256, 64, 64)    (3, 3, 256, 256)\n",
      "64x64/Conv1_down     1180160   (?, 512, 32, 32)    (3, 3, 256, 512)\n",
      "Downscale2D_3        -         (?, 3, 32, 32)      -               \n",
      "FromRGB_lod4         2048      (?, 512, 32, 32)    (1, 1, 3, 512)  \n",
      "Grow_lod3            -         (?, 512, 32, 32)    -               \n",
      "32x32/Conv0          2359808   (?, 512, 32, 32)    (3, 3, 512, 512)\n",
      "32x32/Conv1_down     2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "Downscale2D_4        -         (?, 3, 16, 16)      -               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FromRGB_lod5         2048      (?, 512, 16, 16)    (1, 1, 3, 512)  \n",
      "Grow_lod4            -         (?, 512, 16, 16)    -               \n",
      "16x16/Conv0          2359808   (?, 512, 16, 16)    (3, 3, 512, 512)\n",
      "16x16/Conv1_down     2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "Downscale2D_5        -         (?, 3, 8, 8)        -               \n",
      "FromRGB_lod6         2048      (?, 512, 8, 8)      (1, 1, 3, 512)  \n",
      "Grow_lod5            -         (?, 512, 8, 8)      -               \n",
      "8x8/Conv0            2359808   (?, 512, 8, 8)      (3, 3, 512, 512)\n",
      "8x8/Conv1_down       2359808   (?, 512, 4, 4)      (3, 3, 512, 512)\n",
      "Downscale2D_6        -         (?, 3, 4, 4)        -               \n",
      "FromRGB_lod7         2048      (?, 512, 4, 4)      (1, 1, 3, 512)  \n",
      "Grow_lod6            -         (?, 512, 4, 4)      -               \n",
      "4x4/MinibatchStddev  -         (?, 513, 4, 4)      -               \n",
      "4x4/Conv             2364416   (?, 512, 4, 4)      (3, 3, 513, 512)\n",
      "4x4/Dense0           4194816   (?, 512)            (8192, 512)     \n",
      "4x4/Dense1           513       (?, 1)              (512, 1)        \n",
      "scores_out           -         (?, 1)              -               \n",
      "---                  ---       ---                 ---             \n",
      "Total                23080225                                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "# Load network snapshot\n",
    "##\n",
    "\n",
    "#input_sg_name = \"2019-02-09-stylegan-danbooru2017-faces-network-snapshot-007841.pkl\"\n",
    "\n",
    "# From https://mega.nz/#!vOgj1QoD!GD3E37BroNnZaIR_nic2zVxBtKfAqlvbEC8uBK8-4co\n",
    "input_sg_name = \"2019-02-18-stylegan-faces-network-02041-011095.pkl\"\n",
    "\n",
    "tflib.init_tf()\n",
    "\n",
    "# Load pre-trained network.\n",
    "with open(input_sg_name, 'rb') as f:\n",
    "    # _G = Instantaneous snapshot of the generator. Mainly useful for resuming a previous training run.\n",
    "    # _D = Instantaneous snapshot of the discriminator. Mainly useful for resuming a previous training run.\n",
    "    # Gs = Long-term average of the generator. Yields higher-quality results than the instantaneous snapshot.    \n",
    "    _G, _D, Gs = pickle.load(f)\n",
    "        \n",
    "# Print network details.\n",
    "Gs.print_layers()\n",
    "_D.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbpresent": {
     "id": "c1d6d694-1896-460e-8fe7-3f29ca6a7c97"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Build things on top for encoding\n",
    "# Based on https://github.com/Puzer/stylegan\n",
    "##\n",
    "def create_stub(name, batch_size):\n",
    "    return tf.constant(0, dtype='float32', shape=(batch_size, 0))\n",
    "\n",
    "dlatent_avg = tf.get_default_session().run(Gs.own_vars[\"dlatent_avg\"])\n",
    "def create_variable_for_generator(name, batch_size):\n",
    "    truncation_psi_encode = 0.7\n",
    "    layer_idx = np.arange(16)[np.newaxis, :, np.newaxis]\n",
    "    ones = np.ones(layer_idx.shape, dtype=np.float32)\n",
    "    coefs = tf.where(layer_idx < 8, truncation_psi_encode * ones, ones)\n",
    "    dlatent_variable = tf.get_variable(\n",
    "        'learnable_dlatents', \n",
    "        shape=(1, 16, 512), \n",
    "        dtype='float32', \n",
    "        initializer=tf.initializers.zeros()\n",
    "    )\n",
    "    dlatent_variable_trunc = tflib.lerp(dlatent_avg, dlatent_variable, coefs)\n",
    "    return dlatent_variable_trunc\n",
    "\n",
    "# Generation-from-disentangled-latents part\n",
    "initial_dlatents = np.zeros((1, 16, 512))\n",
    "Gs.components.synthesis.run(\n",
    "    initial_dlatents,\n",
    "    randomize_noise = True, # Turns out this should not be off ever for trying to lean dlatents, who knew\n",
    "    minibatch_size = 1,\n",
    "    custom_inputs = [\n",
    "        partial(create_variable_for_generator, batch_size=1),\n",
    "        partial(create_stub, batch_size = 1)],\n",
    "    structure = 'fixed'\n",
    ")\n",
    "\n",
    "dlatent_variable = next(v for v in tf.global_variables() if 'learnable_dlatents' in v.name)\n",
    "generator_output = tf.get_default_graph().get_tensor_by_name('G_synthesis_1/_Run/G_synthesis/images_out:0')\n",
    "generated_image = tflib.convert_images_to_uint8(generator_output, nchw_to_nhwc=True, uint8_cast=False)\n",
    "generated_image_uint8 = tf.saturate_cast(generated_image, tf.uint8)\n",
    "\n",
    "# Loss part\n",
    "vgg16 = VGG16(include_top=False, input_shape=(512, 512, 3))\n",
    "perceptual_model = keras.Model(vgg16.input, vgg16.layers[9].output)\n",
    "generated_img_features = perceptual_model(preprocess_input(generated_image, mode=\"tf\"))\n",
    "ref_img = tf.get_variable(\n",
    "    'ref_img', \n",
    "    shape = generated_image.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "ref_img_features = tf.get_variable(\n",
    "    'ref_img_features', \n",
    "    shape = generated_img_features.shape,\n",
    "    dtype = 'float32', \n",
    "    initializer = tf.zeros_initializer()\n",
    ")\n",
    "tf.get_default_session().run([ref_img.initializer, ref_img_features.initializer])\n",
    "basic_loss = tf.losses.mean_squared_error(ref_img, generated_image)\n",
    "perceptual_loss = tf.losses.mean_squared_error(ref_img_features, generated_img_features)\n",
    "\n",
    "_D.run(np.zeros((1, 3, 512, 512)), None, custom_inputs = [\n",
    "    lambda x: generator_output,\n",
    "    partial(create_stub, batch_size = 1),\n",
    "])\n",
    "discriminator_output = tf.get_default_graph().get_tensor_by_name('D/_Run/D/scores_out:0')\n",
    "\n",
    "# Attempt at making encoding better: Bias towards mean (\"truncation loss\", essentially)\n",
    "dlatent_avg_full = dlatent_avg.reshape(-1, 512).repeat(16, axis = 0).reshape(-1, 16, 512)\n",
    "input_loss = tf.losses.mean_squared_error(dlatent_variable, dlatent_avg_full)\n",
    "combined_loss = input_loss + perceptual_loss\n",
    "\n",
    "# We literally have a discriminator network, why not use it?\n",
    "discriminator_loss = tf.nn.softplus(-discriminator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "e060a3ca-26b3-4147-977d-8897eab5aa4a"
    }
   },
   "outputs": [],
   "source": [
    "# Gradient descend in latent space to something that is similar to the input image\n",
    "def encode_image(image, iterations = 1024, learning_rate = 0.1, reset_dlatents = True, custom_initial_dlatents = None):\n",
    "    # Get session\n",
    "    sess = tf.get_default_session()\n",
    "    \n",
    "    # Gradient descent initial state\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    optimizer = tf.train.AdadeltaOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(perceptual_loss, var_list=[[dlatent_variable]])\n",
    "    if reset_dlatents == True:\n",
    "        if not custom_initial_dlatents is None:\n",
    "            sess.run(tf.assign(dlatent_variable, custom_initial_dlatents.reshape(-1, 16, 512)))\n",
    "        else:\n",
    "            sess.run(tf.assign(dlatent_variable, initial_dlatents))\n",
    "    \n",
    "    # Generate and set reference image features\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float32\")), [image])))\n",
    "    image_features = perceptual_model.predict_on_batch(preprocess_input(ref_image_data, mode=\"tf\"))  \n",
    "    sess.run(tf.assign(ref_img_features, image_features))\n",
    "    \n",
    "    # Run\n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, perceptual_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "    \n",
    "    # Generate image that actually goes with these dlatents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Same as above but start with given dlatents and use plain MSE loss instead of vgg16\n",
    "def finetune_image(dlatents, image, iterations = 32, learning_rate = 0.0001):\n",
    "    # Get session and assign initial dlatents\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    \n",
    "    # Set reference image\n",
    "    ref_image_data = np.array(list(map(lambda x: (x.astype(\"float64\")), [image])))\n",
    "    sess.run(tf.assign(ref_img, ref_image_data))    \n",
    "    \n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(basic_loss, var_list=[[dlatent_variable]])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, basic_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "\n",
    "    # Generate image that actually goes with these latents for quick testing\n",
    "    dlatents = sess.run(dlatent_variable)[0]\n",
    "    generated_image = generate_images_from_dlatents(dlatents)\n",
    "    \n",
    "    return dlatents, generated_image\n",
    "\n",
    "# Tune image in the direction of being considered more likely by the discriminator\n",
    "def tune_with_discriminator(dlatents, iterations = 32, learning_rate = 1.0):\n",
    "    # Get session and assign initial dlatents\n",
    "    sess = tf.get_default_session()\n",
    "    sess.run(tf.assign(dlatent_variable, np.array([dlatents])))\n",
    "    \n",
    "    # Gradient descent\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    min_op = optimizer.minimize(discriminator_loss, var_list=[[dlatent_variable]])\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        _, loss = sess.run([min_op, basic_loss])\n",
    "        if i % 100 == 0:\n",
    "            print(\"i: {}, l: {}\".format(i, loss))\n",
    "    \n",
    "    return sess.run(dlatent_variable)[0]\n",
    "\n",
    "# We have to do truncation ourselves, since we're not using the combined network\n",
    "def truncate(dlatents, truncation_psi, maxlayer = 8):\n",
    "    dlatent_avg = tf.get_default_session().run(Gs.own_vars[\"dlatent_avg\"])\n",
    "    layer_idx = np.arange(16)[np.newaxis, :, np.newaxis]\n",
    "    ones = np.ones(layer_idx.shape, dtype=np.float32)\n",
    "    coefs = tf.where(layer_idx < maxlayer, truncation_psi * ones, ones)\n",
    "    return tf.get_default_session().run(tflib.lerp(dlatent_avg, dlatents, coefs))\n",
    "\n",
    "# Generate image with disentangled latents as input\n",
    "def generate_images_from_dlatents(dlatents, truncation_psi = 1.0, randomize_noise = True):\n",
    "    if not truncation_psi is None:\n",
    "        dlatents_trunc = truncate(dlatents, truncation_psi)\n",
    "    else:\n",
    "        dlatents_trunc = dlatents\n",
    "        \n",
    "    # Run the network\n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    result_image = Gs.components.synthesis.run(\n",
    "        dlatents_trunc.reshape((-1, 16, 512)),\n",
    "        randomize_noise = randomize_noise,\n",
    "        minibatch_size = 1,\n",
    "        output_transform=fmt\n",
    "    )[0]\n",
    "    return result_image\n",
    "\n",
    "# Sequence of learning steps while reducing lr followed by finetune\n",
    "def encode_and_tune(image, iters_per_step = 1024):\n",
    "    initial_latents = np.random.randn(1, Gs.input_shape[1])\n",
    "    initial_dlatents = Gs.components.mapping.run(initial_latents, None)[0]\n",
    "    dlatents_gen, image_gen = encode_image(image, iterations = iters_per_step, learning_rate = 100.0, custom_initial_dlatents = initial_dlatents)\n",
    "    dlatents_gen2, image_gen2 = encode_image(image, iterations = iters_per_step, learning_rate = 10.0, reset_dlatents = False)\n",
    "    dlatents_gen3, image_gen3 = encode_image(image, iterations = iters_per_step, learning_rate = 1.0, reset_dlatents = False)\n",
    "    dlatents_gen4, image_gen4 = encode_image(image, iterations = iters_per_step, learning_rate = 0.1, reset_dlatents = False)\n",
    "    dlatents_gen5, image_gen5 = encode_image(image, iterations = iters_per_step, learning_rate = 0.01, reset_dlatents = False)\n",
    "    dlatents_gen6, image_gen6 = encode_image(image, iterations = iters_per_step, learning_rate = 0.001, reset_dlatents = False)\n",
    "    dlatents_gen7, image_gen7 = finetune_image(dlatents_gen5, image, iterations = 128)\n",
    "    return dlatents_gen7, image_gen7, dlatents_gen6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4648bf80-75ce-4dd1-beab-bf1b1080d3fc"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 1. Just generate a neat interpolation video\n",
    "##\n",
    "# Pick latent vectors\n",
    "#rnd = np.random.RandomState(5)\n",
    "rnd = np.random\n",
    "latents_a = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_b = rnd.randn(1, Gs.input_shape[1])\n",
    "latents_c = rnd.randn(1, Gs.input_shape[1])\n",
    "\n",
    "if os.path.exists(\"latents.npy\"):\n",
    "    latents_a, latents_b, latents_c = np.load(\"latents.npy\")\n",
    "np.save(\"latents.npy\", np.array([latents_a, latents_b, latents_c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "8073d854-c890-4238-a40e-c2891d071785"
    }
   },
   "outputs": [],
   "source": [
    "# \"Ellipse around a point but probably a circle since it's 512 dimensions\"\n",
    "def circ_generator(latents_interpolate):\n",
    "    radius = 40.0\n",
    "\n",
    "    latents_axis_x = (latents_a - latents_b).flatten() / la.norm(latents_a - latents_b)\n",
    "    latents_axis_y = (latents_a - latents_c).flatten() / la.norm(latents_a - latents_c)\n",
    "\n",
    "    latents_x = math.sin(math.pi * 2.0 * latents_interpolate) * radius\n",
    "    latents_y = math.cos(math.pi * 2.0 * latents_interpolate) * radius\n",
    "\n",
    "    latents = latents_a + latents_x * latents_axis_x + latents_y * latents_axis_y\n",
    "    return latents\n",
    "\n",
    "# Generate images from a list of latents\n",
    "def generate_from_latents(latent_list, truncation_psi):\n",
    "    array_list = []\n",
    "    image_list = []\n",
    "    for latents in latent_list:\n",
    "        # Generate image.\n",
    "        fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "        images = Gs.run(latents, None, truncation_psi=truncation_psi, randomize_noise=False, output_transform=fmt)\n",
    "        array_list.append(images[0])\n",
    "        image_list.append(PIL.Image.fromarray(images[0], 'RGB'))\n",
    "        \n",
    "    return array_list, image_list\n",
    "\n",
    "def mse(x, y):\n",
    "    return (np.square(x - y)).mean()\n",
    "\n",
    "# Generate from a latent generator, keeping MSE between frames constant\n",
    "def generate_from_generator_adaptive(gen_func):\n",
    "    max_step = 1.0\n",
    "    current_pos = 0.0\n",
    "    \n",
    "    change_min = 10.0\n",
    "    change_max = 11.0\n",
    "    \n",
    "    fmt = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    \n",
    "    current_latent = gen_func(current_pos)\n",
    "    current_image = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "    array_list = []\n",
    "    \n",
    "    while(current_pos < 1.0):\n",
    "        array_list.append(current_image)\n",
    "        \n",
    "        lower = current_pos\n",
    "        upper = current_pos + max_step\n",
    "        current_pos = (upper + lower) / 2.0\n",
    "        \n",
    "        current_latent = gen_func(current_pos)\n",
    "        current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "        current_mse = mse(array_list[-1], current_image)\n",
    "        \n",
    "        while current_mse < change_min or current_mse > change_max:\n",
    "            if current_mse < change_min:\n",
    "                lower = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "            \n",
    "            if current_mse > change_max:\n",
    "                upper = current_pos\n",
    "                current_pos = (upper + lower) / 2.0\n",
    "                \n",
    "            \n",
    "            current_latent = gen_func(current_pos)\n",
    "            current_image = images = Gs.run(current_latent, None, truncation_psi=0.5, randomize_noise=False, output_transform=fmt)[0]\n",
    "            current_mse = mse(array_list[-1], current_image)\n",
    "        print(current_pos, current_mse)\n",
    "        \n",
    "    return array_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2f09b5d7-eba0-4bd6-91bb-9c1d816b98fc"
    }
   },
   "outputs": [],
   "source": [
    "#array_list, _ = generate_from_latents(latent_list)\n",
    "array_list = generate_from_generator_adaptive(circ_generator)\n",
    "clip = moviepy.editor.ImageSequenceClip(array_list, fps=60)\n",
    "clip.ipython_display()\n",
    "#clip.write_videofile(\"out.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "dda36a6f-6d51-4bf1-90d5-fcaf845e5b75"
    }
   },
   "outputs": [],
   "source": [
    "arrays, images = generate_from_latents([np.random.randn(1, Gs.input_shape[1])], 0.7)\n",
    "images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "1106645a-f70e-44a2-9ab5-a3cf27bc0810"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 2. Encoding\n",
    "##\n",
    "\n",
    "# Load and cut and scale a bunch of data from the animefaces dataset\n",
    "img_files = []\n",
    "hair_cols = []\n",
    "eye_cols = []\n",
    "for in_dir in glob.glob(\"../../stylegan/animeface-character-dataset/thumb/*\"):\n",
    "    if not os.path.exists(in_dir + \"/color.csv\"):\n",
    "        continue\n",
    "    with open(in_dir + \"/color.csv\", 'r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        for row in csv_reader:\n",
    "            img_files.append(in_dir + \"/\" + row[0])\n",
    "            hair_cols.append([row[1], row[2], row[3]])\n",
    "            eye_cols.append([row[4], row[5], row[6]])\n",
    "img_files = img_files[1::300]\n",
    "hair_cols = hair_cols[1::300]\n",
    "eye_cols = eye_cols[1::300]\n",
    "#print(len(img_files))\n",
    "\n",
    "image_arrays = []\n",
    "for img_file in img_files:\n",
    "    image_data = PIL.Image.open(img_file)\n",
    "    image_size = min(image_data.width, image_data.height)\n",
    "    image_data = image_data.crop((0, 0, image_size, image_size))\n",
    "    image_data = image_data.resize((512, 512), PIL.Image.BILINEAR)\n",
    "    image_array = np.array(image_data)\n",
    "    image_arrays.append(image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f33e2d25-4b1d-4a59-ae5d-c278b9cb98bc"
    }
   },
   "outputs": [],
   "source": [
    "# Encode an image from there\n",
    "dlatents_gen, image_gen = encode_and_tune(image_arrays[0])\n",
    "im = PIL.Image.new('RGB', (1024, 512))\n",
    "im.paste(PIL.Image.fromarray(image_arrays[0], 'RGB'), (0, 0))\n",
    "im.paste(PIL.Image.fromarray(image_gen, 'RGB'), (512, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "2f66010e-db8c-40cc-8587-dee20e54eb64"
    }
   },
   "outputs": [],
   "source": [
    "# Re-encode a generated image \n",
    "# (mind: this is pointless for actual usage, if you're generating you can just \n",
    "# take the latents from the generation step). it's a nice check for encoding, though.\n",
    "generated_ref = generate_from_latents([latents_a])[0][0]\n",
    "dlatents_gen, image_gen = encode_and_tune(generated_ref, iters_per_step = 1024)\n",
    "im = PIL.Image.new('RGB', (1024, 512))\n",
    "im.paste(PIL.Image.fromarray(generated_ref, 'RGB'), (0, 0))\n",
    "im.paste(PIL.Image.fromarray(image_gen, 'RGB'), (512, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "f7fb3de5-155a-4b90-86b6-ceb8f83dabeb"
    }
   },
   "outputs": [],
   "source": [
    "##\n",
    "# 3. Modification\n",
    "##\n",
    "def generate_one():\n",
    "    latents = rnd.randn(1, Gs.input_shape[1])\n",
    "    dlatents = Gs.components.mapping.run(latents, None)[0]\n",
    "    image = generate_images_from_dlatents(dlatents)\n",
    "    return latents, dlatents, PIL.Image.fromarray(image, 'RGB')\n",
    "\n",
    "def classify_image(generated_im):\n",
    "    \"\"\"\n",
    "    there was a function here that used somebodies website to classify images for danbooru tags.\n",
    "    since it's probably better to not have 10 people hit it, I removed it.\n",
    "    the output .pkl for ~6k images is already pretty good to learn directions from and can be found here:\n",
    "    \n",
    "    https://drive.google.com/open?id=1_3Qvhj15bX_pETTENE7THQlfTjx1ghx3\n",
    "    \n",
    "    feel free to put any classifier here.\n",
    "    \"\"\"\n",
    "    return([{\"tag\": likelihood_between_0_and_1}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "caf43dba-3f58-4444-be8a-ed3c0a947f18"
    }
   },
   "outputs": [],
   "source": [
    "latent_list = []\n",
    "dlatent_list = []\n",
    "tag_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "4775c770-0ac4-49f7-a9d2-55a56bfecc96"
    }
   },
   "outputs": [],
   "source": [
    "# Generated and classify a bunch of images\n",
    "while True:\n",
    "    try:\n",
    "        while True:\n",
    "            temp_latents, temp_dlatents, temp_image = generate_one()\n",
    "            temp_tags = classify_image(temp_image)\n",
    "            latent_list.append(temp_latents)\n",
    "            dlatent_list.append(temp_dlatents)\n",
    "            tag_list.append(temp_tags)\n",
    "            print(\"beep\")\n",
    "            \n",
    "            if len(tag_list) % 500 == 0:\n",
    "                with open(\"out_{}.pkl\".format(len(tag_list)), 'wb') as f:\n",
    "                    pickle.dump((latent_list, dlatent_list, tag_list), f)\n",
    "                print(\"Wrote\", \"out_{}.pkl\".format(len(tag_list)))\n",
    "    except:\n",
    "        print(\"nope\")\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "45531cb8-0ee4-4af1-a7ab-354fa0083c1e"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"out_{}.pkl\".format(len(tag_list)), 'wb') as f:\n",
    "    pickle.dump((latent_list, dlatent_list, tag_list), f)\n",
    "print(\"Wrote\", \"out_{}.pkl\".format(len(tag_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "9f162d35-cec6-47e1-9667-43f18dba8d02"
    }
   },
   "outputs": [],
   "source": [
    "# Turn into features for learning directions\n",
    "all_tags = collections.defaultdict(int)\n",
    "for tags in tag_list:\n",
    "    for tag in tags:\n",
    "        all_tags[tag[0]] += 1\n",
    "tags_by_popularity = sorted(all_tags.items(), key = lambda x: x[1], reverse = True)\n",
    "eye_tags = list(filter(lambda x: x[0].endswith(\"_eyes\"), tags_by_popularity))\n",
    "hair_tags = list(filter(lambda x: x[0].endswith(\"_hair\"), tags_by_popularity))\n",
    "\n",
    "tag_binary_feats = {}\n",
    "for tag, _ in tags_by_popularity:\n",
    "    this_tag_feats = []\n",
    "    for tag_list_for_dl in tag_list:\n",
    "        this_dl_tag_value = 0.0\n",
    "        for tag_for_dl, _ in tag_list_for_dl:\n",
    "            if tag == tag_for_dl:\n",
    "                this_dl_tag_value = 1.0\n",
    "        this_tag_feats.append(this_dl_tag_value)\n",
    "    tag_binary_feats[tag] = np.array(this_tag_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "269687f3-8ffb-497e-b9d7-3e4a89ed4535"
    }
   },
   "outputs": [],
   "source": [
    "# Learn directions for tags (some probably not very good)\n",
    "def find_direction_binary(dlatents, targets):\n",
    "    clf = LogisticRegression().fit(dlatents, targets)\n",
    "    return clf.coef_.reshape((16, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "77756bc4-1e18-4e24-b750-a5e4dc8a6383"
    }
   },
   "outputs": [],
   "source": [
    "popular_tags = list(filter(lambda x: x[1] > 100, tags_by_popularity))\n",
    "good_tags = list(filter(lambda x: (len(tag_list) - x[1]) > 1000, popular_tags))\n",
    "\n",
    "dlatents_for_regression = np.array(dlatent_list).reshape(len(dlatent_list), 16*512)\n",
    "tag_directions = {}\n",
    "for i, (tag, _) in enumerate(good_tags):\n",
    "    print(\"Estimating direction for\", tag, \"(\", i, \")\")\n",
    "    tag_directions[tag] = find_direction_binary(dlatents_for_regression, tag_binary_feats[tag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "eea75be9-bcbb-403d-bd20-91b4b8ddd202"
    }
   },
   "outputs": [],
   "source": [
    "#with open(\"tag_dirs.pkl\", 'wb') as f:\n",
    "#    pickle.dump(tag_directions, f)\n",
    "with open(\"tag_dirs.pkl\", 'rb') as f:\n",
    "    tag_directions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "7aabe8f2-71bd-453b-a48f-4bcbdbba3df8"
    }
   },
   "outputs": [],
   "source": [
    "# Do some modification\n",
    "dlatents_gen = Gs.components.mapping.run(latents_a, None)[0]\n",
    "\n",
    "im = PIL.Image.new('RGB', (512 * 5, 512 * 5))\n",
    "for i in range(0, 5):\n",
    "    for j in range(0, 5):\n",
    "        factor_hair = (i / 4.0) * 2.0\n",
    "        factor_eyes = (j / 4.0)\n",
    "    \n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        dlatents_mod += -tag_directions[\"blonde_hair\"] * factor_hair + tag_directions[\"black_hair\"] * factor_hair\n",
    "        dlatents_mod += -tag_directions[\"green_eyes\"] * factor_eyes + tag_directions[\"red_eyes\"] * factor_eyes\n",
    "\n",
    "        dlatents_mod_image = generate_images_from_dlatents(dlatents_mod, 0.7)\n",
    "        im.paste(PIL.Image.fromarray(dlatents_mod_image, 'RGB'), (512 * i, 512 * j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "90a3038e-0d72-4809-88a7-d2ff37a92264"
    }
   },
   "outputs": [],
   "source": [
    "dlatents_gen = Gs.components.mapping.run(latents_c, None)[0]\n",
    "dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "dlatents_mod += -tag_directions[\"purple_hair\"] * 1.0 + tag_directions[\"black_hair\"] * 2.0 - tag_directions[\"green_hair\"]\n",
    "dlatents_mod += -tag_directions[\"blue_eyes\"] * 1.0 + tag_directions[\"green_eyes\"] * 1.0 - tag_directions[\"red_eyes\"]\n",
    "im = PIL.Image.new('RGB', (512 * 2, 512))\n",
    "im.paste(PIL.Image.fromarray(generate_images_from_dlatents(dlatents_gen, 0.7), 'RGB'), (0, 0))\n",
    "im.paste(PIL.Image.fromarray(generate_images_from_dlatents(dlatents_mod, 0.7), 'RGB'), (512, 0))\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "42e1a90f-16db-4216-92d1-8d03af2c7606"
    }
   },
   "outputs": [],
   "source": [
    "lock_updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "nbpresent": {
     "id": "68ca55b8-efe3-49c7-8e64-62f1923d93ea"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1613b663be414bf1bc5b83f8c41e36f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(HBox(children=(Label(value='psi', layout=Layout(width='140px')), FloatSlider(value=0.7, continuoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c2cfc08136243feb4d9772a94dc3cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive modification!\n",
    "hair_eyes_only = False\n",
    "with open(\"tag_dirs.pkl\", 'rb') as f:\n",
    "    tag_directions = pickle.load(f)\n",
    "    \n",
    "tag_len = {}\n",
    "for tag in tag_directions:\n",
    "    tag_len[tag] = np.linalg.norm(tag_directions[tag].flatten())\n",
    "    \n",
    "mod_latents = np.load(\"mod_latents.npy\")\n",
    "dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "def modify_and_sample(psi, truncate_pre, truncate_post, **kwargs):\n",
    "    if truncate_pre == True:\n",
    "        dlatents_mod = truncate(copy.deepcopy(dlatents_gen), psi)\n",
    "    else:\n",
    "        dlatents_mod = copy.deepcopy(dlatents_gen)\n",
    "        \n",
    "    for tag in kwargs:\n",
    "        dlatents_mod += tag_directions[tag] * kwargs[tag]\n",
    "    value_widgets[\"psi\"].value = str(round(psi, 2))\n",
    "    \n",
    "    for tag in kwargs:\n",
    "        tag_value = round((np.dot(dlatents_mod.flatten(), tag_directions[tag].flatten()) / tag_len[tag]) - kwargs[tag], 2)\n",
    "        value_widgets[tag].value = str(kwargs[tag]) + \" | \" + str(tag_value)\n",
    "    \n",
    "    display_psi = None\n",
    "    if truncate_post == True:\n",
    "        display_psi = psi\n",
    "    display(PIL.Image.fromarray(generate_images_from_dlatents(dlatents_mod, truncation_psi = display_psi), 'RGB'))\n",
    "\n",
    "psi_slider = widgets.FloatSlider(min = 0.0, max = 1.0, step = 0.01, value = 0.7, continuous_update = False, readout = False)\n",
    "if hair_eyes_only:\n",
    "    modify_tags = [tag for tag in tag_directions if \"_hair\" in tag or \"_eyes\" in tag or \"_mouth\" in tag]\n",
    "else:\n",
    "    with open(\"tags_use.pkl\", \"rb\") as f:\n",
    "        modify_tags = pickle.load(f)\n",
    "    \n",
    "modify_tags.append(\"realistic\")\n",
    "tag_widgets = {}\n",
    "for tag in modify_tags:\n",
    "    tag_widgets[tag] = widgets.FloatSlider(min = -3.0, max = 3.0, step = 0.01, continuous_update = False, readout = False)\n",
    "all_widgets = []\n",
    "\n",
    "sorted_widgets = sorted(tag_widgets.items(), key = lambda x: x[0])\n",
    "sorted_widgets = [(\"psi\", psi_slider)] + sorted_widgets\n",
    "value_widgets = {}\n",
    "for widget in sorted_widgets:\n",
    "    label_widget = widgets.Label(widget[0])\n",
    "    label_widget.layout.width = \"140px\"\n",
    "    \n",
    "    value_widget = widgets.Label(\"0.0+100.0\")\n",
    "    value_widget.layout.width = \"150px\"\n",
    "    value_widgets[widget[0]] = value_widget\n",
    "    \n",
    "    tag_hbox = widgets.HBox([label_widget, widget[1], value_widget])\n",
    "    tag_hbox.layout.width = \"320px\"\n",
    "    \n",
    "    all_widgets.append(tag_hbox)\n",
    "\n",
    "refresh = widgets.Button(description=\"New Sample\")\n",
    "modify = widgets.Button(description=\"Mutate\")\n",
    "\n",
    "def new_sample(b):\n",
    "    global mod_latents\n",
    "    global dlatents_gen\n",
    "    mod_latents = np.random.randn(1, Gs.input_shape[1])\n",
    "    dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "    psi_slider.value += 0.00000000001\n",
    "    #psi_slider.value -= 0.0000001\n",
    "    \n",
    "def mutate(b):\n",
    "    global mod_latents\n",
    "    global dlatents_gen\n",
    "    mod_latents_add = np.random.randn(1, Gs.input_shape[1]) * 0.1\n",
    "    mod_latents += mod_latents_add\n",
    "    dlatents_gen = Gs.components.mapping.run(mod_latents, None)[0]  \n",
    "    psi_slider.value += 0.00000000001\n",
    "    #psi_slider.value -= 0.0000001\n",
    "\n",
    "truncate_pre = widgets.ToggleButton(value=True, description='Truncate Pre')\n",
    "truncate_post = widgets.ToggleButton(value=False, description='Truncate Post')\n",
    "refresh.on_click(new_sample)\n",
    "modify.on_click(mutate)\n",
    "\n",
    "ui = widgets.Box(all_widgets + [refresh, modify, truncate_pre, truncate_post])\n",
    "tag_widgets[\"psi\"] = psi_slider\n",
    "\n",
    "ui.layout.flex_flow = 'row wrap'\n",
    "ui.layout.display = 'inline-flex'\n",
    "tag_widgets[\"truncate_pre\"] = truncate_pre\n",
    "tag_widgets[\"truncate_post\"] = truncate_post\n",
    "out = widgets.interactive_output(modify_and_sample, tag_widgets)\n",
    "display(ui, out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "fc04a469-4327-4b8d-b3f3-bc6451ca90ba"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"tags_use.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tags, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "190cac07-15b0-4469-831d-a0165d1692fd"
    }
   },
   "outputs": [],
   "source": [
    "tags.remove(\"pokemon_(creature)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "ee587f19-b95d-4365-8ffe-c5aa16045d30"
    }
   },
   "outputs": [],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "3f3593a7-5763-4a0d-ab15-fffd41fe8362"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dlatents_mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-73b283629284>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdlatents_mod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dlatents_mod' is not defined"
     ]
    }
   ],
   "source": [
    "dlatents_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "34e69130-6ff3-45b5-820f-96d847ad8581"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "nbpresent": {
   "slides": {
    "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655": {
     "id": "f7c103f4-722a-48cd-a0e5-e1bc9d0ef655",
     "prev": null,
     "regions": {
      "234da276-ee07-47ef-b598-c4ba844db0b8": {
       "attrs": {
        "height": 1,
        "width": 1,
        "x": -0.0008051529790660225,
        "y": 0
       },
       "content": {
        "cell": "68ca55b8-efe3-49c7-8e64-62f1923d93ea",
        "part": "outputs"
       },
       "id": "234da276-ee07-47ef-b598-c4ba844db0b8",
       "x": 0
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
